{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaad15e-6acb-4045-8d5d-6cd6a5ce2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import format_yelp\n",
    "from LossyCounter import LossyCounter\n",
    "import pygtrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ab1358-16cd-4b28-98a2-1ed092696950",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "max_n = 1\n",
    "lift_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62441834-e4ab-477a-83a5-6073636f9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "train_corpus = dataset['train']['text']\n",
    "test_corpus = dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0b363c-ffdb-4911-8bc3-a7f3381ca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chunk(corpus, skip_words = None):\n",
    "    token_corpus = []\n",
    "    if skip_words is None:\n",
    "        skip_words = set(string.punctuation)# + stopwords.words('english'))\n",
    "    for raw_text in corpus:\n",
    "        formatted_text = format_yelp.format_text(raw_text).lower()\n",
    "        word_list = [word for word in word_tokenize(formatted_text) if word not in skip_words]\n",
    "        #word_list = [word for word in word_list if word not in skip_words]\n",
    "        token_corpus.append(word_list)\n",
    "    return token_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d60e51-abb6-4f95-94aa-0975ce132e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, skip_words = None, chunk_size = 1):\n",
    "    L = len(corpus)\n",
    "    N = math.ceil(L/chunk_size)\n",
    "    pool = Pool()\n",
    "    result = []\n",
    "    with tqdm(total = N, position=0, leave=True) as pbar:\n",
    "        for r in pool.imap(tokenize_chunk,[corpus[i:i+chunk_size] for i in range(0,L,chunk_size)]):\n",
    "            result += r\n",
    "            pbar.update()\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82272d9-2271-437c-9a83-fd9eaffd3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 650000/650000 [01:47<00:00, 6046.35it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:08<00:00, 6237.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_token_corpus = tokenize(train_corpus)\n",
    "test_token_corpus = tokenize(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cdb345-9210-466f-bd2b-78355691405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(word_corpus, eps):\n",
    "    lc = LossyCounter(eps)\n",
    "    for text in tqdm(word_corpus):\n",
    "        lc.cache(text)\n",
    "    lc.flush()\n",
    "    lc.prune()\n",
    "    word_list = lc.getFreqItems()\n",
    "    approx_count = lc.getCounts(word_list, 'median')\n",
    "    result = {}\n",
    "    total = lc.total\n",
    "    sorted_word_list = sorted(word_list, \n",
    "                        key = approx_count.get, \n",
    "                        reverse = True)\n",
    "    for word in sorted_word_list:\n",
    "        result[word] = approx_count[word]/total\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9127af-d4de-41cb-884a-28a474f59ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 650000/650000 [00:09<00:00, 68970.03it/s]\n"
     ]
    }
   ],
   "source": [
    "word_freq = get_word_freq(train_token_corpus, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd44a1d-37f6-43d4-b1bc-6d7e500cf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_freq(corpus, n, eps, batch_size=10):\n",
    "    lc = LossyCounter(eps)\n",
    "    L = len(corpus)\n",
    "    for i in tqdm(range(0,L,batch_size)):\n",
    "        chunk = []\n",
    "        for text in corpus[i:i+batch_size]:\n",
    "            T = len(text)\n",
    "            chunk += ['_'.join(text[i:i+n]) for i in range(T-n+1)]\n",
    "        lc.cache(chunk)\n",
    "    lc.flush()\n",
    "    lc.prune()\n",
    "    ngram_list = lc.getFreqItems()\n",
    "    approx_count = lc.getCounts(ngram_list, 'median')\n",
    "    result = {}\n",
    "    total = lc.total\n",
    "    sorted_ngram_list = sorted(ngram_list, \n",
    "                        key = approx_count.get, \n",
    "                        reverse = True)\n",
    "    for ngram in sorted_ngram_list:\n",
    "        result[ngram] = approx_count[ngram]/total\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12f2a42-1760-4da7-a5b9-ec5d16ee8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq_pool = word_freq.copy()\n",
    "for n in range(2,max_n+1):\n",
    "    ngram_freq = get_ngram_freq(train_token_corpus, n, eps)\n",
    "    ngram_freq_pool = ngram_freq_pool | ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e253c317-ff73-4530-97ae-835324fb586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_prob(phrase, ngram_freq, eps = 1e-7):\n",
    "    words = phrase.split('_')\n",
    "    n = len(words)\n",
    "    baseline = 0\n",
    "    for i in range(1,n):\n",
    "        part1 = '_'.join(words[:i])\n",
    "        if part1 not in ngram_freq and i > 1:\n",
    "            continue\n",
    "        prob1 = ngram_freq.get(part1, eps)\n",
    "        part2 = '_'.join(words[i:])\n",
    "        if part2 not in ngram_freq and i < n-1:\n",
    "            continue\n",
    "        prob2 = ngram_freq.get(part2, eps)\n",
    "        prob = prob1*prob2\n",
    "        if prob > baseline:\n",
    "            baseline = prob\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4ccacb-5312-4dd8-a500-973e92b0ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_vocab(vocab, ngram_freq, scores, threshold = 100):\n",
    "    for ngram in ngram_freq:\n",
    "        if ngram in vocab:\n",
    "            continue\n",
    "        baseline = get_baseline_prob(ngram, ngram_freq)\n",
    "        if baseline == 0:\n",
    "            continue\n",
    "        if ngram_freq[ngram] > threshold * baseline:\n",
    "            vocab[ngram] = ngram_freq[ngram]\n",
    "            scores[ngram] = np.log(ngram_freq[ngram]/baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25578b6b-8410-4571-816e-c7bad4cd34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.copy()\n",
    "scores = {}\n",
    "expand_vocab(vocab, ngram_freq_pool, scores,lift_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a037b2b2-3a4c-4c4c-bb83-e340f52664cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(scores, key = scores.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f214ab-113c-453f-80da-ce722977b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_decode(text, t):\n",
    "    #t = pygtrie.Trie()\n",
    "    #for c in codebook:\n",
    "    #    t[c.split('_')] = codebook[c]\n",
    "    result = []\n",
    "    parsed_text = []\n",
    "    i = 0\n",
    "    L = len(text)\n",
    "    while i < L:\n",
    "        key, value = t.longest_prefix(text[i:])\n",
    "        if key is None:\n",
    "            parsed_text.append(text[i])\n",
    "            i += 1\n",
    "            result.append('UNKNOWN')\n",
    "        else:\n",
    "            parsed_text.append(key)\n",
    "            i += len(key)\n",
    "            result.append(value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3899a714-a88d-454f-919f-bb3e93a9945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pygtrie.Trie()\n",
    "for c in vocab:\n",
    "    t[c.split('_')] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eed9e68b-fb78-42f3-b6f0-08badc75e51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 650000/650000 [07:44<00:00, 1400.70it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [00:32<00:00, 1549.18it/s]\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for text in tqdm(train_token_corpus):\n",
    "    r.append(prefix_decode(text,t))\n",
    "rtest = []\n",
    "for text in tqdm(test_token_corpus):\n",
    "    rtest.append(prefix_decode(text,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54fa7095-5699-41f9-b9d6-95af2295c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51b1dda4-1248-4395-b0a4-5cbb22e2f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=r, vector_size=300, window=5, min_count=1, workers=12, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4191472-0cb2-40bc-be6f-d37856dbe48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7135301828384399),\n",
       " ('horrible', 0.6551201343536377),\n",
       " ('good', 0.6434661746025085),\n",
       " ('awful', 0.6165274977684021),\n",
       " ('stellar', 0.5785626173019409),\n",
       " ('lousy', 0.5498121976852417),\n",
       " ('poor', 0.5445555448532104),\n",
       " ('horrid', 0.5273641347885132),\n",
       " ('crappy', 0.5091362595558167),\n",
       " ('sub-par', 0.5053234100341797)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('bad', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b2f2fd-cc46-4d55-a731-3ea971cee91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = {}\n",
    "for x in vocab:\n",
    "    length[x] = len(x.split('_'))\n",
    "length['UNKNOWN'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9751b9b-5a2b-4063-8826-551036bbd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(phrases, wv, a=1e-4):\n",
    "    vector = [wv[x]*length[x] for x in phrases if x in wv]\n",
    "    if len(vector) == 0:\n",
    "        return np.zeros(300)\n",
    "    result = np.sum(vector,axis=0) / np.sum([length[x] for x in phrases if x in wv],axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aaa007e-bcb9-400d-97e6-bfe413f79fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 650000/650000 [05:04<00:00, 2135.70it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = []\n",
    "label = []\n",
    "for i in tqdm(range(len(r))):\n",
    "    embedding.append(embed(r[i], model.wv))\n",
    "    #label.append(dataset['train']['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edafab89-6933-4e19-a634-6afca1cb54c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [00:23<00:00, 2141.23it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embed = []\n",
    "for i in tqdm(range(len(rtest))):\n",
    "    test_embed.append(embed(rtest[i], model.wv))\n",
    "    #label.append(dataset['train']['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6700b87-e761-4faf-903d-2aa2113cd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0010ff3-c761-4705-ba69-8a4b49523d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = embedding[:500000]\n",
    "ytrain = dataset['train']['label'][:500000]\n",
    "#Xvalid1 = np.array(embedding[450000:500000])\n",
    "#yvalid1 = dataset['train']['label'][450000:500000]\n",
    "Xvalid2 = np.array(embedding[500000:550000])\n",
    "yvalid2 = dataset['train']['label'][500000:550000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "959a681a-8b82-4d51-81dd-dbeb7c2d2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -1.614501\n",
      "[LightGBM] [Info] Start training from score -1.590587\n",
      "[LightGBM] [Info] Start training from score -1.564837\n",
      "[LightGBM] [Info] Start training from score -1.581492\n",
      "[LightGBM] [Info] Start training from score -1.701476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(n_estimators=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(n_estimators=1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcls = lgb.LGBMClassifier(num_leaves = 31, learning_rate = 0.1, n_estimators=1000)\n",
    "bcls.fit(Xtrain, ytrain, eval_set = (Xvalid2, yvalid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80a58201-bea6-4ced-95cc-7fa572c6a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array(test_embed)\n",
    "ytest = np.array(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "279dc176-e532-4391-b7d9-8086627d2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iter = np.argmin(bcls.evals_result_['valid_0']['multi_logloss'])\n",
    "ytesthat = bcls.predict(Xtest, num_iteration = best_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f1f7528-dcd6-45cc-a16f-2ab31bd4d959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "934a8067-bd33-4087-8909-ed2e620e0a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5645"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ytest==ytesthat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "599d414f-f1d3-4a57-9a69-60a84b8c5c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 194041, number of negative: 201398\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 395439, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490698 -> initscore=-0.037214\n",
      "[LightGBM] [Info] Start training from score -0.037214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(n_estimators=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(n_estimators=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(n_estimators=2000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2 = np.where(np.array(ytrain) != 2)[0]\n",
    "index2valid = np.where(np.array(yvalid2) != 2)[0]\n",
    "Xtrain2 = np.array(Xtrain)[index2]\n",
    "ytrain2 = (np.array(ytrain)[index2]>2).astype(int)\n",
    "Xvalid22 = np.array(Xvalid2)[index2valid]\n",
    "yvalid22 = (np.array(yvalid2)[index2valid]>2).astype(int)\n",
    "bcls2 = lgb.LGBMClassifier(num_leaves = 31, learning_rate = 0.1, n_estimators=2000)\n",
    "bcls2.fit(Xtrain2, ytrain2, eval_set = (Xvalid22, yvalid22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "498bc5f1-b00a-4611-a4d3-2ebf85407325",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.array(test_embed)\n",
    "ytest = np.array(dataset['test']['label'])\n",
    "index2test = np.where(np.array(ytest) != 2)[0]\n",
    "Xtest2 = Xtest[index2test]\n",
    "ytest2 = (np.array(ytest)[index2test]>2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "433c9fee-49b8-4824-b249-088e26583447",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iter = np.argmin(bcls2.evals_result_['valid_0']['binary_logloss'])\n",
    "ytesthat = bcls2.predict(Xtest2, num_iteration = best_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9bb3fad-3c0f-4e03-967d-3f52707441db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.919125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ytest2==ytesthat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f967d87-0808-45e7-b563-922222c5980a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1964"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2baad2ea-2b62-4933-9b60-bf75ee5a8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec_lift10.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d851a-7c0c-408a-8ae4-e6da96edb052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
